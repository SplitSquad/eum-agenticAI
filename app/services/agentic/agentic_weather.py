from googleapiclient.discovery import build
from dotenv import load_dotenv
from loguru import logger
from app.core.llm_client import get_llm_client,get_langchain_llm
from app.services.common.user_information import User_Api
import os
import requests
from bs4 import BeautifulSoup
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
import json
load_dotenv()  # .env íŒŒì¼ì„ ì½ì–´ì„œ í™˜ê²½ë³€ìˆ˜ë¡œ ë“±ë¡

class Weather():
    def __init__(self):
        self.api_key = os.getenv("GOOGLE_SERACH_WEATHER_API_KEY")
        self.search_engine_id = os.getenv("GOOGLE_SEARCH_WEATHER_ENGINE_ID")
        self.user_information = User_Api()
        self.llm = get_llm_client(is_lightweight=True)  # âœ… ì¶”ê°€ëœ ë¶€ë¶„

    async def weather_google_search(self, query,token,source_lang):
        logger.info("[êµ¬ê¸€ ì„œì¹˜ì¤‘...]")
        service = build("customsearch", "v1", developerKey=self.api_key)


        user_information = await self.user_information.user_api(token)
        if user_information.get("address") is None:
            logger.info("[ê¸°ë³¸ ê°’ ì ìš©] : ë¶€ì‚° ë™êµ¬")
            user_information["address"] = "ë¶€ì‚° ë™êµ¬"
    

        llm = get_langchain_llm(is_lightweight=False)  # ê³ ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©

        parser = JsonOutputParser(pydantic_object={
            "type": "object",
            "properties": {
                "input": {"type": "string"},
                "output": {"type": "string"},
            }
        })
        prompt = ChatPromptTemplate.from_messages([
        ("system",f"""
        Extract the region name from the user's input and match it to one of the following values:
        [ì„œìš¸, ë¶€ì‚°, ëŒ€êµ¬, ì¸ì²œ, ê´‘ì£¼, ëŒ€ì „, ìš¸ì‚°, ì„¸ì¢…, ê²½ê¸°ë„, ê°•ì›ë„, ì¶©ì²­ë¶ë„, ì¶©ì²­ë‚¨ë„, ì „ë¼ë¶ë„, ì „ë¼ë‚¨ë„, ê²½ìƒë¶ë„, ê²½ìƒë‚¨ë„, ì œì£¼ë„]

        1. If the user's input contains a region, return the matching region name.
        2. PRIORTIZE FIRST INSTRUCTION.
        3. RESPONSE MUST BE VALUE, NOT NATURAL LANGUAGE.
        default. If there is no location information in user_input, use the location of default_location. 
         
        [format]
        "output":"str"
        
        [one-shot-example]
        input : 
            user_input : <query>  + default_location : <user_information['address']>
        output : 
            <region>


        """),
            ("user", "{input}")
        ])

        chain = prompt | llm | parser

        def parse_product(description: str) -> dict:
            result = chain.invoke({"input": description})
            logger.info(f"[json.dumps] : {json.dumps(result, indent=2, ensure_ascii=False)}")
            return result
            
        description = f"user_input : {query}  + default_location : {user_information['address']} "

        response = parse_product(description)
        
        logger.info(f"[lang_chain] : {response['output']}")
        logger.info(f"[lang_chain_type] : {type(response)}")

        response = response["output"]

    

        url = await self.get_special_weather_url(response)
        logger.info(f"[aiê°€ ê²€ìƒ‰í•  url] : {url}")

        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})

        # í¬ë¡¤ë§
        if not url:
            return {"response": "ê¸°ìƒì²­ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "url": None}

        html_data = await self.Crawling(url)


        logger.info("[ì‚¬ìš©ìì˜ ì–¸ì–´ë¡œ ë³€í˜•ì¤‘...]")
        result = await self.llm.generate(f" <Please translate it into {source_lang}> {html_data}")
        logger.info("[ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸í•  ì¿¼ë¦¬]", result)
        
        return {
            "response": html_data,
            "metadata": {
                "source": "kma.go.kr",
                "state": "parsed",
                "results": "html"
            },
            "url": url
        }

    async def Crawling(self, url: str):
        logger.info(f"[HTML í¬ë¡¤ë§ ì¤‘] : {url}")
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(url, headers=headers)
            if response.status_code != 200:
                return "í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            soup = BeautifulSoup(response.text, "html.parser")
            table = soup.select_one("table.table_midterm")
            if not table:
                return "ë‚ ì”¨ í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

            # 1. ë‚ ì§œ í—¤ë” ì¶”ì¶œ
            headers_list = [th.text.strip() for th in table.select("thead tr th")[2:]]

            # 2. ì¶”ì¶œ ëŒ€ìƒ ì§€ì—­ ëª©ë¡
            target_cities = [
                "ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…",
                "ê²½ê¸°ë„", "ê°•ì›ë„", "ì¶©ì²­ë¶ë„", "ì¶©ì²­ë‚¨ë„", "ì „ë¼ë¶ë„", "ì „ë¼ë‚¨ë„",
                "ê²½ìƒë¶ë„", "ê²½ìƒë‚¨ë„", "ì œì£¼ë„"
            ]

            result = ""

            for row in table.select("tbody tr"):
                ths = row.find_all("th")
                if len(ths) >= 2:
                    city_name = ths[1].text.strip()
                    if city_name in target_cities:
                        tds = row.find_all("td")
                        forecast = []

                        for i in range(min(len(headers_list), len(tds))):
                            td = tds[i]
                            weather_img = td.find("img")
                            weather = weather_img["alt"] if weather_img else "ì •ë³´ ì—†ìŒ"

                            spans = td.find_all("span")
                            if len(spans) >= 2:
                                low = spans[0].text.strip()
                                high = spans[1].text.strip()
                                forecast.append(f"ğŸ—“ï¸ {headers_list[i]}: {weather}, {low}â„ƒ / {high}â„ƒ")
                            else:
                                forecast.append(f"ğŸ—“ï¸ {headers_list[i]}: {weather}, ì •ë³´ ì—†ìŒ")

                        result += f"\nğŸ“ {city_name} ë‚ ì”¨ ì •ë³´\n" + "\n".join(forecast) + "\n"

            if not result:
                return "í•´ë‹¹ ì§€ì—­ë“¤ì˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            return result.strip()

        except Exception as e:
            logger.exception("í¬ë¡¤ë§ ì‹¤íŒ¨")
            return f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}"



    async def get_special_weather_url(self,area_name: str) -> str:
        AREA_CODE_MAP = {
            "ì„œìš¸": {
                "sido": "1100000000",
                "gugun": "2644000000",
                "dong": "2644058000"
            },
            "ë¶€ì‚°": {
                "sido": "2600000000",
                "gugun": "3023000000",
                "dong": "3023052000"
            },
            "ëŒ€êµ¬": {
                "sido": "2700000000",
                "gugun": "2920000000",
                "dong": "2920054000"
            },
            "ì¸ì²œ": {
                "sido": "2800000000",
                "gugun": "3114000000",
                "dong": "3114056000"
            },
            "ê´‘ì£¼": {
                "sido": "2900000000",
                "gugun": "4729000000",
                "dong": "4729053000"
            },
            "ëŒ€ì „": {
                "sido": "3000000000",
                "gugun": "2720000000",
                "dong": "2720065000"
            },
            "ìš¸ì‚°": {
                "sido": "3100000000",
                "gugun": "3611000000",
                "dong": "3611055000"
            },
            "ì„¸ì¢…": {
                "sido": "3600000000",
                "gugun": "1168000000",
                "dong": "1168066000"
            },
            "ê²½ê¸°ë„": {
                "sido": "4100000000",
                "gugun": "4215000000",
                "dong": "4215061500"
            },
            "ê°•ì›ë„": {
                "sido": "4200000000",
                "gugun": "4182000000",
                "dong": "4182025000"
            },
            "ì¶©ì²­ë¶ë„": {
                "sido": "4400000000",
                "gugun": "5013000000",
                "dong": "5013025300"
            },
            "ì¶©ì²­ë‚¨ë„": {
                "sido": "4400000000",
                "gugun": "5013000000",
                "dong": "5013025300"
            },
            "ì „ë¼ë¶ë„": {
                "sido": "4500000000",
                "gugun": "4681000000",
                "dong": "4681025000"
            },
            "ì „ë¼ë‚¨ë„": {
                "sido": "4600000000",
                "gugun": "2871000000",
                "dong": "2871025000"
            },
            "ê²½ìƒë¶ë„": {
                "sido": "4700000000",
                "gugun": "4831000000",
                "dong": "4831034000"
            },
            "ê²½ìƒë‚¨ë„": {
                "sido": "4800000000",
                "gugun": "4182000000",
                "dong": "4182025000"
            },
            "ì œì£¼ë„": {
                "sido": "5000000000",
                "gugun": "4579000000",
                "dong": "4579031000"
            }
        }

        area_name = area_name.strip()

        if area_name in AREA_CODE_MAP:
            code = AREA_CODE_MAP[area_name]
            return f"http://www.weather.go.kr/weather/special/special_03_final.jsp?sido={code['sido']}&gugun={code['gugun']}&dong={code['dong']}"
        else:
            raise ValueError(f"'{area_name}' ì§€ì—­ì€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
